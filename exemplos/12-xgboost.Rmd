---
title: "XGBoost"
author: "Athos"
date: "09/07/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse) # metapackage of all tidyverse packages
library(tidymodels)
library(doParallel)
cores = 5
```

## R Markdown


```{r cars}
telco <- readr::read_csv("https://raw.githubusercontent.com/curso-r/intro-ml-mestre/master/dados/telco.csv")
```

## Bases Treini/Teste

```{r}
set.seed(32)
telco_initial_split <- initial_split(telco %>% select(-customerID), prop = 0.8)
telco_train <- training(telco_initial_split)
telco_test <- testing(telco_initial_split)
```

## Exploratória

```{r}
### Por fazer!
```

## Pré-processamento

```{r}
telco_recipe <- recipe(Churn ~ ., telco_train) %>% 
    step_meanimpute(TotalCharges) %>% 
    step_center(all_numeric()) %>% 
    step_scale(all_numeric()) %>% 
    step_dummy(all_nominal(), -all_outcomes())
```


## Definição da Validação Cruzada

```{r}
telco_resamples <- vfold_cv(telco_train, v = 5)
```

## Estratégia de Tunagem de Hiperparâmetros

### Passo 1:

Achar uma combinação `learning_rate` e `trees` que funciona relativamente bem. Usando uma learning_rate alta. Vamos fixar os valores dos outros parâmetros.

- `min_n`: usar um valor entre 1 e 30 é razoável no começo.
- `max_depth`: geralmente começamos com algo entre 4 e 6.
- `loss_reduction`: vamos começar com 0, geralmente começamos com valores baixos.
- `mtry`: começamos com +- 80% do número de colunas na base.
- `sample_size`: também fazemos approx 80% do número de linhas.

Em seguida vamos tunar o `learn_rate` e `trees` em um grid assim:

- `learn_rate` - 0.05, 0.1, 0.3
- `trees` - 100, 500, 1000, 1500

```{r}
telco_model <- boost_tree(
  mtry = 0.8, 
  trees = tune(), 
  min_n = 5, 
  tree_depth = 4,
  loss_reduction = 0, 
  learn_rate = tune(), 
  sample_size = 0.8
) %>% 
  set_mode("classification") %>%
  set_engine("xgboost", nthread = cores)
telco_model
```

#### Workflow

```{r}
telco_wf <- workflow() %>% 
    add_model(telco_model) %>% 
    add_recipe(telco_recipe)
```

#### Grid

```{r}
telco_grid <- expand.grid(
    learn_rate = c(0.05, 0.1, 0.2, 0.3),
    trees = c(100, 250, 500, 1000, 1500, 2000)
)
telco_grid
```

```{r, cache=TRUE}
telco_tune_grid <- telco_wf %>% 
    tune_grid(
     resamples = telco_resamples,
     grid = telco_grid,
     control = control_grid(save_pred = TRUE, verbose = FALSE, allow_par = TRUE),
     metrics = metric_set(roc_auc)
    )
```

#### Melhores hiperparâmetros

```{r}
autoplot(telco_tune_grid)
telco_tune_grid %>% show_best(metric = "roc_auc", n = 5)
telco_select_best_passo1 <- telco_tune_grid %>% select_best(metric = "roc_auc")
telco_select_best_passo1
```


### Passo 2:

Vimos que com os parâmetros da árvore fixos:

- `trees` = `r telco_select_best_passo1$trees`
- `learn_rate` = `r telco_select_best_passo1$learn_rate`

São bons valores inciais. Agora, podemos tunar os parâmetros relacionados à árvore.

- `tree_depth`: vamos deixar ele variar entre 3 e 10.
- `min_n`: vamos deixar variar entre 5 e 90.

Os demais deixamos fixos como anteriormente.

```{r,  cache=TRUE}
telco_model <- boost_tree(
  mtry = 0.8,
  trees = telco_select_best_passo1$trees,
  min_n = tune(),
  tree_depth = tune(), 
  loss_reduction = 0, 
  learn_rate = telco_select_best_passo1$learn_rate, 
  sample_size = 0.8
) %>% 
  set_mode("classification") %>%
  set_engine("xgboost", nthread = cores)

#### Workflow
telco_wf <- workflow() %>% 
    add_model(telco_model) %>% 
    add_recipe(telco_recipe)

#### Grid
telco_grid <- expand.grid(
  tree_depth = c(3, 4, 6, 8, 10), 
  min_n = c(5, 15, 30, 60, 90)
)

telco_tune_grid <- telco_wf %>% 
  tune_grid(
    resamples = telco_resamples,
    grid = telco_grid,
    control = control_grid(save_pred = TRUE, verbose = FALSE, allow_par = TRUE),
    metrics = metric_set(roc_auc)
  )

#### Melhores hiperparâmetros
autoplot(telco_tune_grid)
telco_tune_grid %>% show_best(metric = "roc_auc", n = 5)
telco_select_best_passo2 <- telco_tune_grid %>% select_best(metric = "roc_auc")
telco_select_best_passo2
```

### Passo 3:

Agora temos definidos:

- `trees` = `r telco_select_best_passo1$trees`
- `learn_rate` = `r telco_select_best_passo1$learn_rate`
- `min_n` = `r telco_select_best_passo2$min_n`
- `tree_depth` = `r telco_select_best_passo2$tree_depth`

Vamos então tunar o `loss_reduction`:

`loss_reduction`: vamos deixar ele variar entre 0 e 2


```{r,  cache=TRUE}
telco_model <- boost_tree(
  mtry = 0.8,
  trees = telco_select_best_passo1$trees,
  min_n = telco_select_best_passo2$min_n,
  tree_depth = telco_select_best_passo2$tree_depth, 
  loss_reduction = tune(), 
  learn_rate = telco_select_best_passo1$learn_rate, 
  sample_size = 0.8
) %>% 
  set_mode("classification") %>%
  set_engine("xgboost", nthread = cores)

#### Workflow
telco_wf <- workflow() %>% 
    add_model(telco_model) %>% 
    add_recipe(telco_recipe)

#### Grid
telco_grid <- expand.grid(
  loss_reduction = c(0, 0.05, 0.1, 0.15, 0.25, 0.35, 0.45, 0.5, 1, 2)
)

telco_tune_grid <- telco_wf %>% 
  tune_grid(
    resamples = telco_resamples,
    grid = telco_grid,
    control = control_grid(save_pred = TRUE, verbose = FALSE, allow_par = TRUE),
    metrics = metric_set(roc_auc)
  )

#### Melhores hiperparâmetros
autoplot(telco_tune_grid)
telco_tune_grid %>% show_best(metric = "roc_auc", n = 5)
telco_select_best_passo3 <- telco_tune_grid %>% select_best(metric = "roc_auc")
telco_select_best_passo3
```

### Passo 4:

Não parece que o `lossreduction` teve tanto efeito, mas, vamos usar `r telco_select_best_passo3$loss_reduction` que deu o melhor resultado. Até agora temos definido:


- `trees` = `r telco_select_best_passo1$trees`
- `learn_rate` = `r telco_select_best_passo1$learn_rate`
- `min_n` = `r telco_select_best_passo2$min_n`
- `tree_depth` = `r telco_select_best_passo2$tree_depth`
- `lossreduction` = `r telco_select_best_passo3$loss_reduction`

Vamos então tunar o `mtry` e o `sample_size`: 

- `mtry`: de 10% a 100%
- `sample_size`: de 50% a 100%

```{r}
telco_model <- boost_tree(
  mtry = tune(),
  trees = telco_select_best_passo1$trees,
  min_n = telco_select_best_passo2$min_n,
  tree_depth = telco_select_best_passo2$tree_depth, 
  loss_reduction = telco_select_best_passo3$loss_reduction, 
  learn_rate = telco_select_best_passo1$learn_rate, 
  sample_size = tune()
) %>% 
  set_mode("classification") %>%
  set_engine("xgboost", nthread = cores)

#### Workflow
telco_wf <- workflow() %>% 
    add_model(telco_model) %>% 
    add_recipe(telco_recipe)

#### Grid
telco_grid <- expand.grid(
    sample_size = seq(0.5, 1.0, length.out = 10),
    mtry = seq(0.1, 1.0, length.out = 10)
)

telco_tune_grid <- telco_wf %>% 
  tune_grid(
    resamples = telco_resamples,
    grid = telco_grid,
    control = control_grid(save_pred = TRUE, verbose = FALSE, allow_par = TRUE),
    metrics = metric_set(roc_auc)
  )

#### Melhores hiperparâmetros
autoplot(telco_tune_grid)
telco_tune_grid %>% show_best(metric = "roc_auc", n = 5)
telco_select_best_passo4 <- telco_tune_grid %>% select_best(metric = "roc_auc")
telco_select_best_passo4
```



### Passo 5:

Vimos que a melhor combinação foi

- `mtry` = `r telco_select_best_passo4$mtry`
- `sample_size` = `r telco_select_best_passo4$sample_size`

Agora vamos tunar o `learn_rate` e o `trees` de novo, mas deixando o `learn_rate` assumir valores menores.

```{r}
telco_model <- boost_tree(
  mtry = telco_select_best_passo4$mtry,
  trees = tune(),
  min_n = telco_select_best_passo2$min_n,
  tree_depth = telco_select_best_passo2$tree_depth, 
  loss_reduction = telco_select_best_passo3$loss_reduction, 
  learn_rate = tune(), 
  sample_size = telco_select_best_passo4$sample_size
) %>% 
  set_mode("classification") %>%
  set_engine("xgboost", nthread = cores)

#### Workflow
telco_wf <- workflow() %>% 
    add_model(telco_model) %>% 
    add_recipe(telco_recipe)

#### Grid
telco_grid <- expand.grid(
    learn_rate = c(0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3),
    trees = c(100, 250, 500, 1000, 1500, 2000, 3000)
)

telco_tune_grid <- telco_wf %>% 
  tune_grid(
    resamples = telco_resamples,
    grid = telco_grid,
    control = control_grid(save_pred = TRUE, verbose = FALSE, allow_par = TRUE),
    metrics = metric_set(roc_auc)
  )

#### Melhores hiperparâmetros
autoplot(telco_tune_grid)
telco_tune_grid %>% show_best(metric = "roc_auc", n = 5)
telco_select_best_passo5 <- telco_tune_grid %>% select_best(metric = "roc_auc")
telco_select_best_passo5
```


## Desempenho do Modelo Final

```{r}
telco_model <- boost_tree(
  mtry = telco_select_best_passo4$mtry,
  trees = telco_select_best_passo5$trees,
  min_n = telco_select_best_passo2$min_n,
  tree_depth = telco_select_best_passo2$tree_depth, 
  loss_reduction = telco_select_best_passo3$loss_reduction, 
  learn_rate = telco_select_best_passo5$learn_rate, 
  sample_size = telco_select_best_passo4$sample_size
) %>% 
  set_mode("classification") %>%
  set_engine("xgboost", nthread = cores)

#### Workflow
telco_wf <- workflow() %>% 
    add_model(telco_model) %>% 
    add_recipe(telco_recipe)

telco_last_fit <- telco_wf %>% 
  last_fit(
    split = telco_initial_split,
    control = control_grid(save_pred = TRUE, verbose = FALSE, allow_par = TRUE),
    metrics = metric_set(roc_auc, f_meas)
  )

#### Métricas
collect_metrics(telco_last_fit)

#### Variáveis Importantes
telco_last_fit %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip::vip(num_features = 20)

#### Curva ROC
telco_last_fit %>% 
    collect_predictions() %>% 
    roc_curve(Churn, .pred_Yes) %>% 
    autoplot()
```

